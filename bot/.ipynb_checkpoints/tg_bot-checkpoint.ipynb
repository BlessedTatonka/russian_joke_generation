{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ebdcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7743c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiogram:Bot: russian_jokes_dl [@russian_jokes_dl_bot]\n",
      "WARNING:aiogram:Updates were skipped successfully.\n",
      "INFO:aiogram.dispatcher.dispatcher:Start polling.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from aiogram import Bot, Dispatcher, executor, types\n",
    "import inference as inf\n",
    "from users import *\n",
    "from censore import *\n",
    "\n",
    "# init model\n",
    "tokenizer, model = inf.get_model('../models/GPT2_checkpoint_all_data.pt')\n",
    "lstm_tokenizer, lstm_model = inf.get_lstm_model('../models/lstm_final_epoch=9-step=439270.ckpt')\n",
    "\n",
    "\n",
    "with open('tg_api_token.txt') as tg_api:\n",
    "    TELEGRAM_API_TOKEN = tg_api.read()\n",
    "    \n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Initialize bot and dispatcher\n",
    "bot_state = BotState()\n",
    "bot = Bot(token=TELEGRAM_API_TOKEN)\n",
    "dp = Dispatcher(bot)\n",
    "\n",
    "@dp.message_handler(commands=['help'])\n",
    "async def send_help(message: types.Message):\n",
    "    help_message = ''\n",
    "    with open('help.txt', 'r') as fin:\n",
    "        help_message = fin.read()\n",
    "        \n",
    "    await message.answer(help_message)\n",
    "\n",
    "@dp.message_handler(commands=['params'])\n",
    "async def get_params(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    \n",
    "    result = f'Ваши параметры генерации: \\n{user.params}\\n'\n",
    "    result += f'меньше temperature - менее случайные результаты gpt.\\n'\n",
    "    result += f'больше k - более случайные результаты lstm.'\n",
    "    \n",
    "    \n",
    "    await message.answer(result)\n",
    "    \n",
    "@dp.message_handler(commands=['start'])\n",
    "async def send_start(message: types.Message):\n",
    "#     user = bot_state.get_user_state(message.from_id)\n",
    "    await message.reply(\"Привет! О чём рассказать анекдот?\")\n",
    "\n",
    "@dp.message_handler(commands=['lstm'])\n",
    "async def send_lstm_result(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    \n",
    "    if is_obscene(message.text) is not None:\n",
    "    \n",
    "        result = None\n",
    "        # Фильтр мата\n",
    "        while result is None:\n",
    "            result = inf.get_lstm_prediction(f'{message.text}', lstm_tokenizer, lstm_model, \\\n",
    "                                             params_dict=user.params)\n",
    "            result = is_obscene(result)\n",
    "            \n",
    "    else:\n",
    "        result = 'В запросе есть плохое слово!'\n",
    "            \n",
    "    await message.reply(result)\n",
    "    \n",
    "@dp.message_handler(commands=['change_param'])\n",
    "async def change_param(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    print(user)\n",
    "\n",
    "    try:\n",
    "        query = message.text.split(' ')\n",
    "        param = query[1]\n",
    "        value = float(query[2])\n",
    "        \n",
    "        if param not in user.params.keys():\n",
    "            result = 'Такого параметра не существует.'\n",
    "        else:\n",
    "            user.params[param] = value\n",
    "        \n",
    "        result = f'Значение параметра {param} изменено на {value}.'\n",
    "    except:\n",
    "        result = 'Запрос должен иметь вид: /change_param <param> <value>'\n",
    "    \n",
    "    await message.reply(result)\n",
    "    \n",
    "@dp.message_handler()\n",
    "async def send_gpt_result(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    \n",
    "    if is_obscene(message.text) is not None:\n",
    "    \n",
    "        result = None\n",
    "        # Фильтр мата\n",
    "        while result is None:\n",
    "            result = inf.get_prediction(f'Расскажи анекдот {message.text}', tokenizer, model, \\\n",
    "                                        params_dict=user.params)\n",
    "            result = is_obscene(result)\n",
    "            \n",
    "    else:\n",
    "        result = 'В запросе есть плохое слово!'\n",
    "            \n",
    "    await message.reply(result)\n",
    "    await message.answer(\"О чём ещё рассказать анекдот?\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    executor.start_polling(dp, skip_updates=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
