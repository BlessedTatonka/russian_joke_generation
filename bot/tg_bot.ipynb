{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ebdcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7743c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aiogram:Bot: russian_jokes_dl [@russian_jokes_dl_bot]\n",
      "WARNING:aiogram:Updates were skipped successfully.\n",
      "INFO:aiogram.dispatcher.dispatcher:Start polling.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from aiogram import Bot, Dispatcher, executor, types\n",
    "import inference as inf\n",
    "from users import *\n",
    "from censore import *\n",
    "\n",
    "# init model\n",
    "tokenizer, model = inf.get_model('../models/GPT2_checkpoint_all_data.pt')\n",
    "lstm_tokenizer, lstm_model = inf.get_lstm_model('../models/lstm_final_epoch=9-step=439270.ckpt')\n",
    "\n",
    "\n",
    "with open('tg_api_token.txt') as tg_api:\n",
    "    TELEGRAM_API_TOKEN = tg_api.read()\n",
    "    \n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Initialize bot and dispatcher\n",
    "bot_state = BotState()\n",
    "bot = Bot(token=TELEGRAM_API_TOKEN)\n",
    "dp = Dispatcher(bot)\n",
    "\n",
    "@dp.message_handler(commands=['help'])\n",
    "async def send_help(message: types.Message):\n",
    "    help_message = ''\n",
    "    with open('help.txt', 'r') as fin:\n",
    "        help_message = fin.read()\n",
    "        \n",
    "    await message.answer(help_message)\n",
    "\n",
    "@dp.message_handler(commands=['params'])\n",
    "async def get_params(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    \n",
    "    result = f'Ваши параметры генерации: \\n{user.params}\\n'\n",
    "    result += f'меньше temperature - менее случайные результаты gpt.\\n'\n",
    "    result += f'больше k - более случайные результаты lstm.'\n",
    "    \n",
    "    \n",
    "    await message.answer(result)\n",
    "    \n",
    "@dp.message_handler(commands=['start'])\n",
    "async def send_start(message: types.Message):\n",
    "#     user = bot_state.get_user_state(message.from_id)\n",
    "    await message.reply(\"Привет! О чём рассказать анекдот?\")\n",
    "\n",
    "@dp.message_handler(commands=['lstm'])\n",
    "async def send_lstm_result(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    \n",
    "    if is_obscene(message.text) is not None:\n",
    "    \n",
    "        result = None\n",
    "        # Фильтр мата\n",
    "        while result is None:\n",
    "            result = inf.get_lstm_prediction(f'{message.text}', lstm_tokenizer, lstm_model, \\\n",
    "                                             params_dict=user.params)\n",
    "            result = is_obscene(result)\n",
    "            \n",
    "    else:\n",
    "        result = 'В запросе есть плохое слово!'\n",
    "            \n",
    "    await message.reply(result)\n",
    "    \n",
    "@dp.message_handler(commands=['change_param'])\n",
    "async def change_param(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    print(user)\n",
    "\n",
    "    try:\n",
    "        query = message.text.split(' ')\n",
    "        param = query[1]\n",
    "        value = float(query[2])\n",
    "        \n",
    "        if param not in user.params.keys():\n",
    "            result = 'Такого параметра не существует.'\n",
    "        else:\n",
    "            user.params[param] = value\n",
    "        \n",
    "        result = f'Значение параметра {param} изменено на {value}.'\n",
    "    except:\n",
    "        result = 'Запрос должен иметь вид: /change_param <param> <value>'\n",
    "    \n",
    "    await message.reply(result)\n",
    "    \n",
    "@dp.message_handler()\n",
    "async def send_gpt_result(message: types.Message):\n",
    "    user = bot_state.get_user_state(message.from_id)\n",
    "    \n",
    "    if is_obscene(message.text) is not None:\n",
    "    \n",
    "        result = None\n",
    "        # Фильтр мата\n",
    "        while result is None:\n",
    "            result = inf.get_prediction(f'Расскажи анекдот {message.text}', tokenizer, model, \\\n",
    "                                        params_dict=user.params)\n",
    "            result = is_obscene(result)\n",
    "            \n",
    "    else:\n",
    "        result = 'В запросе есть плохое слово!'\n",
    "            \n",
    "    await message.reply(result)\n",
    "    await message.answer(\"О чём ещё рассказать анекдот?\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    executor.start_polling(dp, skip_updates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3218f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from censore import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef616f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "привет\n",
      "ривет \n",
      "ивет г\n",
      "вет го\n",
      "ет гов\n",
      "т говн\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет\n",
      "ривет \n",
      "ивет г\n",
      "вет го\n",
      "ет гов\n",
      "т говн\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет\n",
      "ривет \n",
      "ивет г\n",
      "вет го\n",
      "ет гов\n",
      "т говн\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет го\n",
      "ривет гов\n",
      "ивет говн\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "прив\n",
      "риве\n",
      "ивет\n",
      "вет \n",
      "ет г\n",
      "т го\n",
      " гов\n",
      "говн\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет\n",
      "ривет \n",
      "ивет г\n",
      "вет го\n",
      "ет гов\n",
      "т говн\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет г\n",
      "ривет го\n",
      "ивет гов\n",
      "вет говн\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет го\n",
      "ривет гов\n",
      "ивет говн\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет гов\n",
      "ривет говн\n",
      "ивет говно\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет гов\n",
      "ривет говн\n",
      "ивет говно\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет г\n",
      "ривет го\n",
      "ивет гов\n",
      "вет говн\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет г\n",
      "ривет го\n",
      "ивет гов\n",
      "вет говн\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет го\n",
      "ривет гов\n",
      "ивет говн\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет го\n",
      "ривет гов\n",
      "ивет говн\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет го\n",
      "ривет гов\n",
      "ивет говн\n",
      "вет говно\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет г\n",
      "ривет го\n",
      "ивет гов\n",
      "вет говн\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет г\n",
      "ривет го\n",
      "ивет гов\n",
      "вет говн\n",
      "ет говно\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "привет \n",
      "ривет г\n",
      "ивет го\n",
      "вет гов\n",
      "ет говн\n",
      "т говно\n",
      " говно\n",
      "говно\n",
      "овно\n",
      "вно\n",
      "но\n",
      "о\n",
      "приве\n",
      "ривет\n",
      "ивет \n",
      "вет г\n",
      "ет го\n",
      "т гов\n",
      " говн\n",
      "говно\n"
     ]
    }
   ],
   "source": [
    "is_obscene('Привет говно')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03d6858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Штирлиц часто подумывал, что это - новая пытка Мюллера.- Не думал, что ты ее придумал, Штирлиц.\"|'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split('|1|-|')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899b3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference as inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53a18443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def get_model(checkpoint):\n",
    "    model_name = \"DeepPavlov/rudialogpt3_medium_based_on_gpt2_v2\"   \n",
    "    tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    model.load_state_dict(torch.load(checkpoint)['model_state_dict'])\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def get_prediction(query, tokenizer, model):\n",
    "    # INFERENCE\n",
    "\n",
    "    chat_history_ids = torch.zeros((1, 0), dtype=torch.int)\n",
    "    new_user_input_ids = tokenizer.encode(f\"|0|1|{query}{tokenizer.eos_token}\", return_tensors=\"pt\")\n",
    "    chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "\n",
    "    new_user_input_ids = tokenizer.encode(f\"|1|-|\", return_tensors=\"pt\")\n",
    "    chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "    input_len = chat_history_ids.shape[-1]\n",
    "\n",
    "    chat_history_ids = model.generate(\n",
    "        chat_history_ids.cuda(),\n",
    "        num_return_sequences=1,                     # use for more variants, but have to print [i]\n",
    "        max_length=512,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        temperature = 0.5,                          # 0 for greedy\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "        \n",
    "    result = f\"{tokenizer.decode(chat_history_ids[0].cpu().detach(), skip_special_tokens=True)}\"\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbfd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_model('GPT2_checkpoint_all_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa8cd4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|0|1|Расскажи анекдот про Вовочку|1|3|\"- Вовочка, как тебя зовут?- Не знаю, я еще маленький.- Почему?- А зачем?\"|'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction('Расскажи анекдот про Вовочку', tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfddf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# !git clone https://github.com/dariush-bahrami/character-tokenizer.git\n",
    "import string\n",
    "import sys\n",
    "from charactertokenizer import CharacterTokenizer\n",
    "\n",
    "# from torch.utils.data import TensorDataset, random_split\n",
    "# from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59495a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_chars.txt', 'r') as fin:\n",
    "    chars = fin.read()\n",
    "    \n",
    "model_max_length = 512\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length, padding_size='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c95151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn as nn\n",
    "# from transformers import BertModel\n",
    "\n",
    "\n",
    "# Класс модели (через лайтнинг!)\n",
    "# LSTM 3 слоя размера 1024 -> два линейных слоя \n",
    "\n",
    "class LSTM(pl.LightningModule):\n",
    "    def __init__(self, n_vocab):\n",
    "        super().__init__()        \n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1024, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear_1 = nn.Linear(1024, 512)\n",
    "        self.relu = nn.ELU()\n",
    "        self.linear_2 = nn.Linear(512, n_vocab)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "        \n",
    "        loss = self.loss_fn(x, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.forward(x)\n",
    "        \n",
    "        loss = self.loss_fn(x, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float().unsqueeze(-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.linear_1(self.dropout(x))\n",
    "        x = self.linear_2(self.relu(self.dropout(x)))\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=2e-4)\n",
    "        return optimizer\n",
    "\n",
    "lstm = LSTM(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db93cd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a224bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 1024, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (linear_1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (relu): ELU(alpha=1.0)\n",
       "  (linear_2): Linear(in_features=512, out_features=315, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = lstm.load_from_checkpoint(checkpoint_path=\"../models/lstm_final_epoch=9-step=439270.ckpt\", n_vocab=tokenizer.vocab_size)\n",
    "lstm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f3c06cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"[CLS]Штирлиц\"\n",
      " может быть, потом перестальное которого трудно слава 'си' спросил на ней без коробе поворачивается с ней на работать?[SEP]\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "pattern = torch.tensor(tokenizer.encode('Штирлиц'))[:-1]\n",
    "    \n",
    "lstm.eval()\n",
    "lstm.cuda()\n",
    "print('Prompt: \"%s\"' % tokenizer.decode(pattern))\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        x = pattern / float(tokenizer.vocab_size)\n",
    "        prediction = lstm(x.unsqueeze(0).cuda().float())\n",
    "        p = softmax(prediction * k).cpu().flatten().detach().numpy()\n",
    "        index = np.random.choice(np.arange(len(prediction.flatten())), 1, p=p)\n",
    "        result = tokenizer.decode(index)\n",
    "        print(result, end=\"\")\n",
    "        if index == 1:\n",
    "            break\n",
    "        pattern = torch.concatenate((pattern.cuda(), torch.tensor(index).cuda()))\n",
    "        pattern = pattern[1:]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff6c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_class import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
