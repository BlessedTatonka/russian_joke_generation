{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147ded4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"anek.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "\n",
    "# raw_text = '|X|'.join([item.strip() for item in raw_text.split('* * *')])\n",
    "# raw_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85e07313",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Лошадь может смотреть на все 360 градусов. \\nА твой батя только на 40.',\n",
       " '— Дорогая, я... \\n— Нет никаких \"я\" или \"ты\". Есть только \"мы\". \\n— Хорошо. Дорогая, мы обосрались.',\n",
       " '— Ты вел себя, как джентельмен. Спасибо, что не лапал меня. \\n— Не за что.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [item.strip() for item in raw_text.split('***')][1:]\n",
    "samples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c848724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    samples[i] = \" \".join(re.findall(\"[а-я]+\", samples[i].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5c01ad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' ', 'о', 'а', 'е', 'т', 'и', 'н', 'с', 'р', 'л', 'в', 'к', 'у',\n",
       "       'д', '\\n', 'м', 'п', '.', ',', 'я', 'ь', 'ы', 'г', 'б', 'з', 'ч',\n",
       "       'й', '*', 'ж', '-', 'ш', 'х', 'ю', ':', '!', '?', '—', 'ц', 'П',\n",
       "       'Н', 'В', 'А', 'С', 'щ', 'О', 'Д', 'ё', 'М', '\"', 'Т', 'э', 'И',\n",
       "       'К', 'ф', 'Е', 'Р', 'Б', 'У', 'З', 'Ч', 'Л', 'Я', 'Г', '–', '0',\n",
       "       'e', '1', 'a', 'Х', 'Э', 'o', '«', '»', 'Ш', '2', 'p', 'ъ', 'Ж',\n",
       "       '5'], dtype='<U1')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "values, counts = np.unique(list(raw_text), return_counts=True)\n",
    "values[np.argsort(-counts)][:79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7782bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da6ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d99dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/dariush-bahrami/character-tokenizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "04600564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "from charactertokenizer import CharacterTokenizer\n",
    "\n",
    "chars = ''.join(values[np.argsort(-counts)][:100])\n",
    "# chars = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя ' \n",
    "model_max_length = 12651\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length, padding_size='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f406ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterTokenizer(name_or_path='', vocab_size=107, model_max_length=12651, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0f93067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  12651\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in samples:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8a671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_LENGTH = max_len\n",
    "for sent in samples:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(samples)\n",
    "\n",
    "print('Original: ', samples[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70e638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# chars = sorted(list(set(raw_text)))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    " \n",
    "seq_length = 20\n",
    "dataX = []\n",
    "dataY = []\n",
    "    \n",
    "for sample in input_ids:\n",
    "    \n",
    "    n_chars = len(sample)\n",
    "#     n_vocab = len(chars)\n",
    "\n",
    "    for i in range(0, n_chars - seq_length):\n",
    "        seq_in = sample[i:i + seq_length]\n",
    "        seq_out = sample[i + seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "        \n",
    "        if seq_out == 1 or seq_out == 4:\n",
    "            break\n",
    "    n_patterns = len(dataX)\n",
    "\n",
    "X = torch.stack(dataX)\n",
    "# X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd083246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "93709d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "# from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class LSTM(pl.LightningModule):\n",
    "    def __init__(self, n_vocab):\n",
    "        super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained('dbmdz/bert-tiny-historic-multilingual-cased')\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=512, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear_1 = nn.Linear(512, 512)\n",
    "        self.relu = nn.ELU()\n",
    "        self.linear_2 = nn.Linear(512, n_vocab)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "#         x = x.float()\n",
    "        x = self.forward(x)\n",
    "        \n",
    "        loss = self.loss_fn(x, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "#         x = x.float()\n",
    "        x = self.forward(x)\n",
    "        \n",
    "        loss = self.loss_fn(x, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float().unsqueeze(-1)\n",
    "#         x = self.bert(x)['pooler_output']\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.linear_1(self.dropout(x))\n",
    "        x = self.linear_2(self.relu(self.dropout(x)))\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "lstm = LSTM(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b78fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "# bert = BertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c3c617ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = bert.forward(torch.tensor([[1, 2, 3]]))\n",
    "# a['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7d269c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_loader:\n",
    "    x, y = item\n",
    "    lstm.forward(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "24eb8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=64, num_workers=12)\n",
    "val_loader = utils.data.DataLoader(val_dataset, batch_size=64, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd9b1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"models/\", save_top_k=2, monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29fa7678",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/boris/anaconda3/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/boris/Documents/ML_COURSES/dl-course/Final_project/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type             | Params\n",
      "----------------------------------------------\n",
      "0 | lstm     | LSTM             | 3.2 M \n",
      "1 | dropout  | Dropout          | 0     \n",
      "2 | linear_1 | Linear           | 262 K \n",
      "3 | relu     | ELU              | 0     \n",
      "4 | linear_2 | Linear           | 21.0 K\n",
      "5 | loss_fn  | CrossEntropyLoss | 0     \n",
      "----------------------------------------------\n",
      "3.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.4 M     Total params\n",
      "13.759    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fb2684c31846c49eecff891de6ce8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model=lstm, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "266b2ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([16, 29, 25, 39, 40, 23, 22, 12, 15, 11, 22,  9, 40, 16, 40, 21,  7, 10,\n",
      "        21, 27,  9, 32, 16, 25, 36, 40, 18, 40,  8, 19, 12, 25, 26, 39, 33, 12,\n",
      "        17, 40,  9, 40, 19, 27, 31,  7, 29, 40,  9, 22, 25, 29]), tensor(22))\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    print(item)\n",
    "    tmp, _ = item\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c276fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d5aa436",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"[CLS]штирлиц\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ь закрывали корова и отправить на следующий вот пришлости пришел в сторонка по раз у него пришел в такой по стоит как только себе да ты сегодня так и говорит на ебать кричит почел вернуть не потом поставились корова отпусти лет в этом как тебе понимает и говорит лежит но тебя ему стоит не совсем верхипнализации в клиник гостини и спрашивать самое с разбили да на следующий в какой и сразу пришли по окончивый спрашивать и вот так попросит и спрашиваться ну показывали так на следуют пришел подходители а там что то как ты не было пооробой сразу голосом покажу отпусти но не ответил и говорить в сотси не понимает старый солдаты себя нет в монастый сынок не постуча человеч как на работы закрывать такой какое молодецкой а ты вот себе и стоит пролаза с конциков по рука и так на это воздь заказали то старую после не продавать поровы папа на нее с фуротали открывает на месте и как ты что так и спрашиваться на кухню на члены сидит самомер и говорит он так на половинется в тараканинов вы то что так вот так и смотри не смотрик и она отпусти на волоса на столько отпусти на запомник и студенный корова вопрос не понимает провери так и она посмотр поручик и в израились сидит покупать открыл проклять вот ты вы то что такой так тоже посмотре в моей женщины в сторон да не собрал в зале вот это положил заходит становки приходичь ну што по простил приклад так сказал не поверили в кровать снова пришита не смотри приш л мол он все в головой на раз и вот кто то охициантов вы что так только после такой и начал не поводительности они на концил за получит на положить себя старик на кухня и спрашивается заключи не затем так вы не получит на какой женщины вы закусились и спрашивать в стороны поднима только подходители да немного не выготовита стоит не может с отпусти открыл с ну и потом ну как только вы стали так в раз не забрал и говорить в комнаты и так и лучший клад тут разрешись закончик и сказал слова а тот со стою так просто вы животный кирпичествоваты и отправил по пропил закрыл солдать подходится мужик пока на лицо скажет проходит на концил под некоторный приехали мужик и вот получить сидит совсем на вот своего которые на положились на доктор отпусти так вы так в кресло все чувствоваты и говорить пришел отпусти в приезжа в кармика вот ты не подумать с руку и она у него разорваты у него по колосово приступы после открывает как так вот смотри как вот ты чего только по подумали по одного спрашивает колису в кармана и стоит когда такие корова и спрашивается такого как так и пошел они в головой и верхиповал не понять подходить на следующий пришел на следую на комбидали в лесу и ответил в ад ну ответильность молодой мужик на концили все на кустов заходить на какие и вот вот сказал молодецком и вот стоит и спрашиваться на следую в какой то себя который как ты кто только с собес по водительный и открывает с него и вот сосет на поле так видит закупит к молодой а ты не понимает и просто с колосной прошит говорит сказал на приходик милола с которые они подходить в головой делать прошло по они в это женщина сын заключи показывает и молодец не заходить на колобок пришли не усталовити коровку и вот нахаласонес на стольк и начинает на голос в али да вот так под ошень пришлости в кусты а ты кто то что так и всего и не закрывает и вот так на корова открывает приколом с уракточку и метров волк на простит в карман и планетили мужик пришел то блять а там не подош лет и спрашиваться одну как на головую как так быть коровую на следую видит закрывает коротком есть армянин и говорить какие пришел выбрал и пошел вопросит пока так и прости и так в карман не понимает своего мужик один из картину и приходичь сказали простил от стоит прошит и спрашивается в раз и спрашиваться на следующий отпусти на планит в том стало закончик под положить и спрашивается с ороровые так вот так я е в конца и вот с корично приходичь сила вокруга не может в одну приезжа ну и так и на человек ну показали как то с наблюды с ней вот с какой показал ему как тебя покупать да нет нет ты по плохой за милиционал положил и присоешали с поровет с колосным десять так покитик как только приш л молодецко стоит и спрашиваться и вот ты что ты не понимает в полосились и студении поставил смотри на следующий по стоит в разлет и смотри такой в концеты совершит с крикную поручик на следующего то попробовал на самого половинов с немецки со слово простил на одно он приш л не запреще прошит которые тебе попробовать стоит приступы корова признак в какой открывать доктор и после собака по лес на работататалки его миллиона с ней видит на понимает а там не после вот поставили с нас не подарил как ты не пора открыл пришел в приставил и говорить один из них сидит приходить с разрезался покажу приш л нахал да ну на картину и говорится вы старухий по по водительно в красивый вопросы сержанный принима разрабаться в комнатов тебя открывает в зале в бар и только да давай а ну ты время пришли за стоит пришел чувствоватил в карман а там не попробовать и заказывать в ней подошел не подош лыбите пара в сторон стало у него открывать на следующий и начинается на положить заходить вот ты не выбрали и сказал с продавций в доктор на верхипы и так прошит так от какие поручик остановки открыл открывает покажи как ты по волна и пошит и покажить в бар и прикрыв не открывается и принима и вот ты что такой тоже то скакал мужик проходит да ты по слепой да о том чтобы из одного и вот вот смотри за стоит да тебе приш л того которые высот у меня не после вот открывает пришел на карита при это сказал слушай отличное спрашивается так вот как ты вы не скажи через приходит в   не вышел в концерти то что это подарил как так с разрези и поставил на комесов давай подходителей никто на путин и говориться складывается комдун и спрашивает на следующий и он на следующий отпусти по получить такое так видит такого из за колис мужик когда вот сказал раз в нашей открыв открывает и спрашиваться нет по волоса как ты не подходится свой поручик подошли и спрашивает по одна плать купи такой такой в ответил и вот вы попалостью и проклять по красиво когда ебануты и просит ну закончик с ней покажу через да ты как ты такой сказал и тут с какой так и попробование и говорит мужик заходить он все старый закрывает близ разрешись хоть пока и вот дальше видит так и в красивы просто и корова по конкурсионали три конгок и так после по воспита скажи как так в приклад подходители по лицо на кустов на король спрашиваться человеч по головую просит на колиссяс одного спрашивает по вопроса спрашивать в руках и так на доктор и говорит к немуерки водительно раздался старик заказывается п том чтобы не говорится ну и после открывает и в комнаты на следующий мы смотри слова в прошитель срус только приветом на положил а там не подходители в головой отрусти самое такой не повериться в колен проходить он на местедь а то вашему только временыкий скажите подходит корова половин старый на классии вот так и давайте на столька старшик на положил приходик по голосом и давай вот сделать как времени один из картину потом открывает фватается и простились охициант студений у него приходил на кусты с чукча по алкогола в карман и вот не понимает и так показали не заходит и корова и самое и просто отрубить на положить с который в кармана поручик мужик по слово так на балкон стал как когда один из как так и спрашивает в прид т к немусок и один говорит с одного через на какой спросил у него приш л приказа совсемие и очередь а ты сказал старый на одно слушай открывает и приходичь по следую корочек доктор а потом после открывает вы приш л плохо поручик рукой по лицо расскаже на голосов и пока и попробовать в кармана из конкурсно да со свои как из кадом в различно положить на полиций открывается в карман по положили обратно простом и говорить да ты сказал на то чтобы на следующий к немузик нет ты все так же открывает один из как вот мне но подали в приезжает концил пришел на камень нет ну как сидит да тоже так да нет ты опять чего то взял нам нет пришел с каратист по руку привет в россии сказал и скажу сидит сразу с крышка на кухок мальчик и давайте с концили и говорит в одному через написать от как он приш л билет против раз просто поручик на кухня не попробовать поползат и спрашиваться вы вот плечами и говорит мило купил отпусти закончик в приезжа купил ну к немурок с ней в карман об уголоток и как то вот руку показать прошит с концу приступы в него в какой скажу через нам такого телефон и тут немного стоит говорится подходители ну как ты как это вы знает открывает поручик после на слепой не попробовать и так он подходители открывается на магоня и закрывает старую по водки слова после как то на с"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ледующий мы вы не видел ну пока ответил по следую у него сталинов и начинается отпусти когда так и закончик пришло и вот не подходители и тогда в прошит в комнаты на прошительно подошли открывается положить потом поручик и закрывает пока и сказалась совсем так вы сделать по лошадь и приходик да пришел в этом покупать по одного подходить ну вот не понимает пришел в двери старый в комнаты скажи на положить так в партии и простол и спрашивает передали заходит к тебе он постучать на улицка постучу товариваю а вот видит заказ по получить какие у него приш л ну на прикрыв скажи на четыре пришел на курить и спрашиваются был в бар пожило он в команды и говорить ну давайтельно и он закрыл и кричит пришел в один из прошить ты васили в него подходить ну что вы скажи через мужик господа и накил только проходит стоит и говорится на стольки медведь на сторожить ну и стоит по да вы скажи как ты не задал он стало как в сраку на следующий как ты как вот смотри пришел тебе в купил закрывает и говорит да вот пропить подходит в одной голосом покажи наш просто кружку ответил и так и спрашивается слушай на концил и говорит человек и так на балкон и монахину и не понимает и при стоит стало молодецкой и начинается такой на королей и пришел в трахать с него по однажды как ты пришел в комнаты на заднужт сидит подходить на колобок не не стало не видит а там не получить на достается на который пришлостью на который и так был ты чего поровертить ему с просит от мужик и пришел как ты не под корова половинство пришел пришел закрича только он говорится на концили да ну и сказал разверный красиво на после старшик ответить и говорить так вот после один из тебя в прошу как тебя закрывали меня стало делать только подумали в красивый пришел потом получить на одну вернуть медведь пришел по москалит на следующий приш л не ложит слушай на это показывает по руку спрашивает и как ты потом руки приходик так приезжает на следую на концилов за получится в постепей открывает ну и спрашивай вот ты чего телего руку бармен ответил по плакать пришлос на них конский в этом когда не подстава женщина все стало приходик он покупает на концила пришел а там такой машину и говорит сразу на голос и расскаже не закрывать был ты получит как весь говорит на лесу в монастыю вы не понимает в столет и спрашиваться по порови по ноги надоел так и вот смотри с прошит и показали и не отвечает корова на какой должен и говорить самое и покажу что то вижу в перед тебе с ним в сраку и спрашиваться как обратно пропол закончик и пошел и потом когда на следующий как то закрывает в удивляет приш л скажи открывает один из конечно так что выбрать с двери стоит и говорит в концертво приезжали показать и пришлость не заметил написанин как ты открывает после так вы что только старый в постели собак в постепи не приходит а там стоит и вот есть приш л тебе как ты закрывает пока не выдали ответил открывает вот на какой нет это такого котором в состояние пришел и то после смотри говорить вы стало и спрашивает ему потом спросил не видит так слона жена по водку сразу закрывать в кости такой в концерт открыл плачу так так и пока не спрашивать в одной в "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">31</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 #         x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = pattern                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># generate logits as output from the model</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>31 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>prediction = lstm(x.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).cuda().float())                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># convert logits into one character</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>p = softmax(prediction * k).cpu().flatten().detach().numpy()                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>index = np.random.choice(np.arange(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(prediction.flatten())), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, p=p)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/boris/.local/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">42</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x):                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x.float().unsqueeze(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 #         x = self.bert(x)['pooler_output']</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>42 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x, _ = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lstm(x)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">43 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x[:, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, :]                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.linear_1(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout(x))                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.linear_2(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.relu(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout(x)))                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/boris/.local/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/boris/.local/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">rnn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">812</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 809 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 810 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.check_forward_args(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, hx, batch_sizes)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 811 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> batch_sizes <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 812 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>result = _VF.lstm(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, hx, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._flat_weights, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_layers,  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 813 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bidirectional, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.batc  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 814 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 815 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>result = _VF.lstm(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, batch_sizes, hx, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._flat_weights, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias,      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m31\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m#         x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   │   \u001b[0mx = pattern                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# generate logits as output from the model\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m31 \u001b[2m│   │   \u001b[0mprediction = lstm(x.unsqueeze(\u001b[94m0\u001b[0m).cuda().float())                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m32 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# convert logits into one character\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   \u001b[0mp = softmax(prediction * k).cpu().flatten().detach().numpy()                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   │   \u001b[0mindex = np.random.choice(np.arange(\u001b[96mlen\u001b[0m(prediction.flatten())), \u001b[94m1\u001b[0m, p=p)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/boris/.local/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m42\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x):                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m40 \u001b[0m\u001b[2m│   │   \u001b[0mx = x.float().unsqueeze(-\u001b[94m1\u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m#         x = self.bert(x)['pooler_output']\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m42 \u001b[2m│   │   \u001b[0mx, _ = \u001b[96mself\u001b[0m.lstm(x)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m│   │   \u001b[0mx = x[:, -\u001b[94m1\u001b[0m, :]                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m44 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.linear_1(\u001b[96mself\u001b[0m.dropout(x))                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.linear_2(\u001b[96mself\u001b[0m.relu(\u001b[96mself\u001b[0m.dropout(x)))                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/boris/.local/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/boris/.local/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mrnn.py\u001b[0m:\u001b[94m812\u001b[0m in \u001b[92mforward\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 809 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 810 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.check_forward_args(\u001b[96minput\u001b[0m, hx, batch_sizes)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 811 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m batch_sizes \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 812 \u001b[2m│   │   │   \u001b[0mresult = _VF.lstm(\u001b[96minput\u001b[0m, hx, \u001b[96mself\u001b[0m._flat_weights, \u001b[96mself\u001b[0m.bias, \u001b[96mself\u001b[0m.num_layers,  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 813 \u001b[0m\u001b[2m│   │   │   │   │   │   │     \u001b[0m\u001b[96mself\u001b[0m.dropout, \u001b[96mself\u001b[0m.training, \u001b[96mself\u001b[0m.bidirectional, \u001b[96mself\u001b[0m.batc  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 814 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 815 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = _VF.lstm(\u001b[96minput\u001b[0m, batch_sizes, hx, \u001b[96mself\u001b[0m._flat_weights, \u001b[96mself\u001b[0m.bias,      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Generation using the trained model\n",
    "# best_model, char_to_int = torch.load(\"single-char.pth\")\n",
    "# n_vocab = len(char_to_int)\n",
    "# int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "# model.load_state_dict(best_model)\n",
    " \n",
    "# randomly generate a prompt\n",
    "# seq_length = 100\n",
    "# filename = \"jokes.txt\"\n",
    "# raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "# raw_text = raw_text.lower()\n",
    "# raw_text = raw_text.replace('* * *', '').replace('\\n\\n', '')\n",
    "# start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "# prompt = raw_text[start:start+seq_length]\n",
    "# prompt = 'Привет'\n",
    "# pattern = [char_to_int[c] for c in prompt]\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "pattern = torch.tensor(tokenizer.encode('штирлиц'))[:-1]\n",
    "    \n",
    "lstm.eval()\n",
    "lstm.cuda()\n",
    "print('Prompt: \"%s\"' % tokenizer.decode(pattern))\n",
    "with torch.no_grad():\n",
    "    for i in range(100000):\n",
    "        # format input array of int into PyTorch tensor\n",
    "#         x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "        x = pattern\n",
    "        # generate logits as output from the model\n",
    "        prediction = lstm(x.unsqueeze(0).cuda().float())\n",
    "        # convert logits into one character\n",
    "        p = softmax(prediction * k).cpu().flatten().detach().numpy()\n",
    "        index = np.random.choice(np.arange(len(prediction.flatten())), 1, p=p)\n",
    "#         index = [np.argmax(p)]\n",
    "        result = tokenizer.decode(index)\n",
    "        print(result, end=\"\")\n",
    "        if index == 1:\n",
    "            break\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        pattern = torch.concatenate((pattern.cuda(), torch.tensor(index).cuda()))\n",
    "        pattern = pattern[1:]\n",
    "print()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b46087ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0611e-09, 4.5398e-05, 9.9995e-01]])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(torch.tensor([[1, 2, 3]]).float() * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8446f0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25,  7, 18,  7, 20, 10, 14,  9, 13, 11, 13, 12, 18,  9, 22,  7, 17, 28,\n",
       "        38,  8, 20, 12, 11,  7,  8, 60, 12, 44, 10, 15,  7, 12,  7, 29,  8, 17,\n",
       "         8, 15, 12, 11, 40,  7, 55, 45,  9, 15, 13, 12, 25,  7, 22, 28,  7, 16,\n",
       "        10, 11, 12, 22,  7, 17,  7, 48, 60, 29,  9, 13, 12, 14, 11,  9, 13, 55,\n",
       "        25,  7, 23,  9, 15, 13, 12,  7, 23,  8, 22, 15,  9, 32, 13, 10, 16, 12,\n",
       "        25,  7, 13,  8,  7,  8, 60, 12, 44, 10])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "17a160eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c5de190c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'р'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lstm(tmp.unsqueeze(0).cuda().float()).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "36a599f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25,  7, 18,  7, 20, 10, 14,  9, 13, 11, 13, 12, 18,  9, 22,  7, 17, 28,\n",
       "        38,  8, 20, 12, 11,  7,  8, 60, 12, 44, 10, 15,  7, 12,  7, 29,  8, 17,\n",
       "         8, 15, 12, 11, 40,  7, 55, 45,  9, 15, 13, 12, 25,  7, 22, 28,  7, 16,\n",
       "        10, 11, 12, 22,  7, 17,  7, 48, 60, 29,  9, 13, 12, 14, 11,  9, 13, 55,\n",
       "        25,  7, 23,  9, 15, 13, 12,  7, 23,  8, 22, 15,  9, 32, 13, 10, 16, 12,\n",
       "        25,  7, 13,  8,  7,  8, 60, 12, 44, 10])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
